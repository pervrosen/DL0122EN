{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src=\"https://ibm.box.com/shared/static/qo20b88v1hbjztubt06609ovs85q8fau.png\" width=\"400px\" align=\"center\"></a>\n",
    "<br>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">GPU vs CPU</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we learn how to run a program on different devices.\n",
    "\n",
    "There are usually multiple computing devices available on each machine. TensorFlow, supports three types of devices:\n",
    "<ul>\n",
    "    <li>\"/cpu:0\": The CPU of your machine.</li>\n",
    "    <li>\"/gpu:0\": The GPU of your machine, if you have one.</li>\n",
    "    <li>\"/device:XLA:0\": Optimized domain-specific compiler for linear algebra.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.log_device_placement=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>List of CPU and GPUs</h3>\n",
    "How to get list of CPU and GPUs on your machine ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:CPU:0', '/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    with tf.Session(config=config) as sess:\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see one CPU, one or more GPUs if it is available on your computer, and also XLA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>Performance of GPU vs CPU</h4>\n",
    "<p>Most of Deep Learning models, especially in their training phase, involve a lot of matrix and vector multiplications that can parallelized. In this case, GPUs can outperform CPUs, because GPUs were designed to handle these kind of matrix operations in parallel!</p>\n",
    "\n",
    "<h4>Why GPU outperforms?</h4>\n",
    "<p>A single core CPU performs a matrix operation in serial, one element at a time. But, a single GPU could have hundreds or thousands of cores, while a CPU typically has no more than a few cores.</p>\n",
    "\n",
    "\n",
    "<h4>How to use GPU with TensorFlow?</h4>\n",
    "<p>It is important to notice that if both CPU and GPU are available on the machine that you are running a notebook, and if a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device.</p>\n",
    "\n",
    "<p>In our case, as we are running this notebook on <a href=\"http://cocl.us/ML0122EN_IBMCLOUD_PowerAI\">IBM PowerAI</a>, you have access to multi GPU, but lets use one of the GPUs in this notebook, for the sake of simplicity.</p>\n",
    "\n",
    "<h4>What is XLA?</h4>  \n",
    "<p>XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that optimizes TensorFlow computations. The results are improvements in speed, memory usage, and portability on server and mobile platforms. Initially, most users will not see large benefits from XLA, but are welcome to experiment by using XLA via just-in-time (JIT) compilation or ahead-of-time (AOT) compilation. Developers targeting new hardware accelerators are especially encouraged to try out XLA.</p>\n",
    "\n",
    "<p>The XLA framework is experimental and in active development. In particular, while it is unlikely that the semantics of existing operations will change, it is expected that more operations will be added to cover important use cases. The team welcomes feedback from the community about missing functionality and community contributions via GitHub.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logging device</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>It is recommended to use <b>logging device placement</b> when using GPUs, as this lets you easily debug issues relating to different device usage. This prints the usage of devices to the log, allowing you to see when devices change and how that affects the graph. Unfortunately, the current version of Jupyter notebook does not show the logs properly, but still you can print out the nodes as a json file and check the devices. It will work fine if your script is running outside of Jupyter notebook though.</p>\n",
    "\n",
    "<p>You can see that a, b and c are all run on GPU0.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n",
      "[node {\n",
      "  name: \"MatMul\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\000\\000\\260A\\000\\000\\340A\\000\\000DB\\000\\000\\200B\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"MatMul/_0\"\n",
      "  op: \"_Send\"\n",
      "  input: \"MatMul\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"client_terminated\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"recv_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device_incarnation\"\n",
      "    value {\n",
      "      i: 1\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"tensor_name\"\n",
      "    value {\n",
      "      s: \"edge_4_MatMul\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "library {\n",
      "}\n",
      "versions {\n",
      "  producer: 26\n",
      "}\n",
      ", node {\n",
      "  name: \"MatMul/_1\"\n",
      "  op: \"_Recv\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"client_terminated\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"recv_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device\"\n",
      "    value {\n",
      "      s: \"/job:localhost/replica:0/task:0/device:GPU:0\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"send_device_incarnation\"\n",
      "    value {\n",
      "      i: 1\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"tensor_name\"\n",
      "    value {\n",
      "      s: \"edge_4_MatMul\"\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"tensor_type\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"_retval_MatMul_0_0\"\n",
      "  op: \"_Retval\"\n",
      "  input: \"MatMul/_1\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"index\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "library {\n",
      "}\n",
      "versions {\n",
      "  producer: 26\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def print_logging_device():\n",
    "    # Creates a graph.\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.constant([1.0, 2.0, 3.0, 4.0, ], shape=[2, 2], name='c')\n",
    "    mu = tf.matmul(a, b)\n",
    "    # Creates a session with log_device_placement set to True.\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # Runs the op.\n",
    "        options = tf.RunOptions(output_partition_graphs=True)\n",
    "        metadata = tf.RunMetadata()\n",
    "        c_val = sess.run(mu, options=options, run_metadata=metadata)\n",
    "        print (c_val)\n",
    "        print (metadata.partition_graphs)\n",
    "\n",
    "print_logging_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What types of operations should I send to the GPU?</h4>\n",
    "\n",
    "<p>Basically, if a step of the process encompass “do this mathematical operation many times”, then it is a good candidate operation to send it to be run on the GPU. For example,</p>\n",
    "<ul>\n",
    "    <li>Matrix multiplication</li>\n",
    "    <li>Computing the inverse of a matrix</li>\n",
    "    <li>Gradient calculation, which are computed on multiple GPUs individually and are averaged on CPU</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Device placement:</h3>\n",
    "<p>As mentioned, by default, GPU get priority for the operations which support running on it. But, you can manually place an operation on a device to be run. You can use <b>tf.device</b> to assign the operations to a device context.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23. 30.]\n",
      " [52. 68.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a new graph.\n",
    "myGraph = tf.Graph()\n",
    "with tf.Session(config=config, graph=myGraph) as sess:\n",
    "    with tf.device('/gpu:0'):\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "        c = tf.constant([1.0, 2.0, 3.0, 4.0, ], shape=[2, 2], name='c')\n",
    "        mu = tf.matmul(a, b)\n",
    "    with tf.device('/cpu:0'):\n",
    "        ad = tf.add(mu,c)\n",
    "    print (sess.run(ad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use one of Alex Mordvintsev deep dream [notebook]() to visualize the above graph. You can change the color to see the operations assigned to GPU and CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.4738523292002518&quot;).pbtxt = 'node {\\n  name: &quot;a&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:GPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\200?\\\\000\\\\000\\\\000@\\\\000\\\\000@@\\\\000\\\\000\\\\200@\\\\000\\\\000\\\\240@\\\\000\\\\000\\\\300@&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;b&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:GPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\200?\\\\000\\\\000\\\\000@\\\\000\\\\000@@\\\\000\\\\000\\\\200@\\\\000\\\\000\\\\240@\\\\000\\\\000\\\\300@&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;c&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:GPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\200?\\\\000\\\\000\\\\000@\\\\000\\\\000@@\\\\000\\\\000\\\\200@&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;a&quot;\\n  input: &quot;b&quot;\\n  device: &quot;/device:GPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;MatMul&quot;\\n  input: &quot;c&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.4738523292002518&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper functions for TF Graph visualization\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "  \n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "  \n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "graph_def = myGraph.as_graph_def()\n",
    "tmp_def = rename_nodes(graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multiplication on GPU and CPU</h3>\n",
    "In the following cell, we define a function, to measure the speed of matrix multiplication in CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def matrix_mul(device_name, matrix_sizes):\n",
    "    time_values = []\n",
    "    #device_name = \"/cpu:0\"\n",
    "    for size in matrix_sizes:\n",
    "        with tf.device(device_name):\n",
    "            random_matrix = tf.random_uniform(shape=(1000,1000), minval=0, maxval=1)\n",
    "            dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "            sum_operation = tf.reduce_sum(dot_operation)\n",
    "\n",
    "        with tf.Session(config=config) as session:\n",
    "            startTime = datetime.now()\n",
    "            result = session.run(sum_operation)\n",
    "        td = datetime.now() - startTime\n",
    "        time_values.append(td.microseconds/1000)\n",
    "        print (\"matrix shape:\" + str(size) + \"  --\"+ device_name +\" time: \"+str(td.microseconds/1000))\n",
    "    return time_values\n",
    "\n",
    "\n",
    "matrix_sizes = range(100,2000,100)\n",
    "time_values_gpu = matrix_mul(\"/gpu:0\", matrix_sizes)\n",
    "time_values_cpu = matrix_mul(\"/cpu:0\", matrix_sizes)\n",
    "print (\"GPU time\" +  str(time_values_gpu))\n",
    "print (\"CPU time\" + str(time_values_cpu))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the performance of training the model on a CPU versus on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(matrix_sizes[:len(time_values_gpu)], time_values_gpu, label='GPU')\n",
    "plt.plot(matrix_sizes[:len(time_values_cpu)], time_values_cpu, label='CPU')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('Size of Matrix ')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// Shutdown kernel\n",
    "Jupyter.notebook.session.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>When should not use GPU?</h4>\n",
    "\n",
    "<p>GPUs do not have direct access to the rest of your computer (except, of course for the display). Due to this, if you are running a command on a GPU, you need to copy all of the data to the GPU first, then do the operation, then copy the result back to your computer’s main memory. TensorFlow handles this under the hood, so the code is simple, but the work still needs to be performed.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Do you want to use GPU in production?</h2>\n",
    "\n",
    "<p>Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The <a href=\"https://cocl.us/ML0122EN_IBMCLOUD_PowerAI\">PowerAI platform on IBM Cloud</a> supports popular machine learning libraries and dependencies including TensorFlow, Caffe, PyTorch, and Theano.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Thanks for completing this lesson!</h3>\n",
    "\n",
    "<h4>Author: <a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a></h4>\n",
    "<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients’ ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "</article>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<p>Copyright &copy; 2018 <a href=\"https://cocl.us/DX0108EN_CC\">Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
